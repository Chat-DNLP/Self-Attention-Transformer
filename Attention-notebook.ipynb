{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica: implementación del mecanismo de auto-atención con enmascaramiento del modelo Transformer\n",
    "\n",
    "Vamos a implementar el mecanismo de auto-atención con enmascaramiento del modelo Transformer en Pytorch. Para ello, vamos a seguir los pasos descritos anteriormente y suponer que ya tenemos las matrices de consultas (Q), claves (K) y valores (V) para cada token en la secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "Q = torch.tensor([[0.0, 0.0, 0.0], [1, 1, 1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3]])\n",
    "K = torch.tensor([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3], [0.4, 0.4, 0.4]])\n",
    "V = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El enmascaramiento durante la etapa del decodificador en los modelos Transformer es crucial para evitar que el decodificador tenga acceso a información futura, especialmente en tareas de generación secuencial como la traducción automática o la generación de texto. Este concepto se conoce como \"enmascaramiento de atención causal\".\n",
    "\n",
    "En el contexto de los Transformers, el decodificador genera una salida secuencialmente, palabra por palabra. Durante la generación de cada palabra, es importante que el modelo solo tenga en cuenta las palabras anteriores y no las futuras, ya que estas últimas no deberían estar disponibles (en un escenario de generación de texto, por ejemplo, las palabras futuras aún no se han generado).\n",
    "\n",
    "Una vez realizado el resultado debe ser:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><b>z1</b></td><td>1.0000</td><td>0.0000</td><td>0.0000</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>z2</b></td><td>0.4568</td><td>0.5432</td><td>0.0000</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>z3</b></td><td>0.3219</td><td>0.3332</td><td>0.3449</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>z4</b></td><td>0.2309</td><td>0.5130</td><td>0.5260</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "#### **Objetivos de la práctica**\n",
    "\n",
    "- Entender con detalle el funcionamiento del mecanismo de auto-atención con enmascaramiento.\n",
    "- Practicar las operaciones matriciales en PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
